# 第一章 马尔可夫决策过程

概念自测: Markov Decision Process的定义、Markov性质、State、State space、Action、State transition及其条件概率描述、Policy及其条件概率描述、Reward及其条件概率描述、Return、Discounted return、Episode。

***

## 一、什么是 Markov Decision Process

- 马尔可夫决策过程（MDP）是一种用来描述“在不确定环境中反复做决策并累积回报”的数学模型，是强化学习的标准建模方式。
- 一个离散时间的随机控制过程，每一步智能体根据当前环境做决策，环境以概率方式转移到下一个状态并给出奖励。
- MDP常用一个五元组表示：$\langle \mathcal{S},\mathcal{A},\mathcal{P},\mathcal{R},\gamma \rangle$。其中 $\mathcal{S}$ 是状态集合，$\mathcal{A}$ 是动作集合，$\mathcal{P}$ 是状态转移概率，$\mathcal{R}$ 是奖励函数，$\gamma$ 是折扣因子。

***

## 二、Markov 性质（马尔可夫性质）

- 核心句：**未来只取决于现在，与过去无关**。形式上是
$P(S_{t+1}\mid S_t, S_{t-1},\dots) = P(S_{t+1}\mid S_t)$
- 在 MDP 里进一步要求：在给定当前状态与动作时，下一时刻的状态与奖励只依赖当前这一步：
$P(S_{t+1},R_{t+1}\mid S_t,A_t,S_{t-1},A_{t-1},\dots) = P(S_{t+1},R_{t+1}\mid S_t,A_t)$。

***

## 三、State 与 State Space

- **State（状态）**：对当前环境“足够充分”的描述，使得在该描述下过程具有马尔可夫性，例如网格世界中的坐标位置、游戏中的棋盘局面等。
- **State Space（状态空间）**：所有可能状态的集合，一般记为 $\mathcal{S}$，可以是有限集合，也可以是连续空间（例如位置、速度是连续变量）。

***

## 四、Action 与 Policy

- **Action（动作）**：智能体在某个状态下可选择的操作，例如上/下/左/右、加速/刹车等；某个状态 $s$ 下可用动作集合记为 $\mathcal{A}(s)$。
- **Policy（策略）**：给定当前状态时，智能体选择各个动作的规则，通常写成条件概率
$\pi(a\mid s) = P(A_t=a\mid S_t=s)$，表示在状态 $s$ 下选动作 $a$ 的概率。

***

## 五、状态转移与其条件概率

- **State Transition（状态转移）**：在时间步 $t$ 处于状态 $S_t=s$，执行动作 $A_t=a$ 后，以一定概率转移到下一个状态 $S_{t+1}=s'$。
- 用条件概率描述为：
$P(s'\mid s,a) = P(S_{t+1}=s'\mid S_t=s, A_t=a)$，这就是 MDP 中的转移模型 $\mathcal{P}$。

***

## 六、Reward 及其条件概率

- **Reward（奖励）**：环境在转移过程中给智能体的标量反馈，用来度量“好坏”，通常记为 $R_{t+1}$。]
- 奖励可以用条件期望或分布来描述，例如
$R(s,a) = \mathbb{E}[R_{t+1}\mid S_t=s, A_t=a]$ 或 $R(s,a,s') = \mathbb{E}[R_{t+1}\mid S_t=s,A_t=a,S_{t+1}=s']$。

***

## 七、Return 与 Discounted return

- **Return（回报）**：从某一时间步开始，把之后所有奖励累加起来的总和，一般记为
$G_t = R_{t+1} + R_{t+2} + R_{t+3} + \dots$。
- **Discounted return（折扣回报）**：为了让远期奖励“打折”并保证无穷和收敛，引入折扣因子 $\gamma \in [0,1)$，定义
$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots$。

***

## 八、Episode

- 在许多任务中，一个完整的交互过程从起始状态出发，经过有限步决策，最终到达一个终止状态，这样的状态–动作–奖励序列称为一个 episode（轨迹）。
- 以时间步记为：$S_0, A_0, R_1, S_1, A_1, R_2, \dots, S_T$，其中 $S_T$ 是终止状态；在 episodic 任务中，回报往往以这条轨迹的累计奖励来评估策略好坏。

***