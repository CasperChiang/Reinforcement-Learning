# 第二章 贝尔曼公式

## State Value 函数

- **State Value $v_\pi(s)$**：在策略 $\pi$ 下，从状态 $s$ 开始的期望 return，即 $v_\pi(s) = \mathbb{E}_\pi[G_t \mid S_t = s]$，其中 $G_t$ 是路径的回报。

## Deterministic情况下的State Value计算

- **Deterministic 策略（确定性策略）**：给定状态 $s$ 时，总输出同一个动作，例如 $\pi(s) = a$，不带随机性；在板书例子中，策略总是选固定动作。
- **Stochastic 策略（随机性策略）**：给定状态 $s$ 时，输出动作的概率分布，例如 $\pi(a|s) = P(A=a|S=s)$，允许随机选择以探索；在实际 RL 中常用随机策略。

- 推导: 从不同状态 $s_i$（$i=1,2,3,4$）出发的折扣 return：
$$
\begin{cases}
v_1 = r_1 + \gamma r_2 + \gamma^2 r_3 + \dots \\
v_2 = r_2 + \gamma r_3 + \gamma^2 r_4 + \dots \\
v_3 = r_3 + \gamma r_4 + \gamma^2 r_1 + \dots \\
v_4 = r_4 + \gamma r_1 + \gamma^2 r_2 + \dots 
\end{cases}
$$
&emsp;&emsp;这里 $r_i$ 是从 $s_i$ 执行策略后得到的即时奖励。
$$
\begin{cases}
v_1 = r_1 + \gamma (r_2 + \gamma r_3 + \dots) = r_1 + \gamma v_2 \\
v_2 = r_2 + \gamma (r_3 + \gamma r_4 + \dots) = r_2 + \gamma v_3 \\
v_3 = r_3 + \gamma (r_4 + \gamma r_1 + \dots) = r_3 + \gamma v_4 \\
v_4 = r_4 + \gamma (r_1 + \gamma r_2 + \dots) = r_4 + \gamma v_1 
\end{cases}
$$

&emsp;&emsp;括号里的部分就是“下一状态的 return”，因为策略确定，转移确定。

&emsp;&emsp;将以上写成矩阵：

$$
\begin{bmatrix}
v_1 \\ v_2 \\ v_3 \\ v_4 
\end{bmatrix}
=
\begin{bmatrix} r_1 \\ r_2 \\ r_3 \\ r_4 \end{bmatrix}
+
\gamma
\begin{bmatrix}
0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1 \\
1 & 0 & 0 & 0 
\end{bmatrix}
\begin{bmatrix}
v_1 \\ v_2 \\ v_3 \\ v_4 
\end{bmatrix}
$$

&emsp;&emsp;记 $\mathbf{v} = \mathbf{r} + \gamma P \mathbf{v}$，其中 $P$ 是确定策略下的转移矩阵。



&emsp;&emsp;移项得 $\mathbf{v} = (I - \gamma P)^{-1} \mathbf{r}$。

## 贝尔曼公式(deterministic情况下)

- 对state value的迭代：$$v_\pi(s) = r(s, \pi(s)) + \gamma v_\pi(s')，$$这就是贝尔曼公式。
